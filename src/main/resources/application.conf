app {
  env = "Local"   # "Local" | "Hadoop"

  // chemin racine du projet dans ta machine
  local.root = ${?user.home}"/projets_scala/Flight_project_JMEA"

  input {
    // Données brutes (tu as tout mis "en local" désormais)
    flights.dir = ${app.local.root}"/data/flights"           # ex: 201201.csv, etc.
    weather.dir = ${app.local.root}"/data/weather"           # ex: 201201hourly.txt
    mapping     = ${app.local.root}"/data/wban_airport_timezone.csv"
    months_f    = ["201201"]                                 # élargir plus tard
    months_w    = ["201201"]
  }

  output {
    // En local on remplace dbfs:/delta/... par un dossier delta/ dans le projet
    delta.base.bronze = ${app.local.root}"/delta/bronze"
    delta.base.silver = ${app.local.root}"/delta/silver"
    delta.base.gold   = ${app.local.root}"/delta/gold"
  }

  // Profil Hadoop (exemple HDFS)
  hadoop {
    input.flights.dir = "hdfs:///projects/flight/data/flights"
    input.weather.dir = "hdfs:///projects/flight/data/weather"
    input.mapping     = "hdfs:///projects/flight/data/wban_airport_timezone.csv"

    output.delta.base.bronze = "hdfs:///delta/bronze"
    output.delta.base.silver = "hdfs:///delta/silver"
    output.delta.base.gold   = "hdfs:///delta/gold"
  }

  params {
    thMinutes = 60
    missingness.threshold = 0.60
  }
}

spark {
  master = "local[*]"     # ou "yarn"
  appName = "Flight_Project_JMEA"

  // Nécessaire à Delta Lake
  sql.extensions = "io.delta.sql.DeltaSparkSessionExtension"
  sql.catalog    = "org.apache.spark.sql.delta.catalog.DeltaCatalog"

  conf = {
    "spark.sql.adaptive.enabled" = "true"
    "spark.sql.adaptive.skewJoin.enabled" = "true"
    "spark.sql.files.maxPartitionBytes" = "134217728"
  }
}
